# -*- coding: utf-8 -*-
"""Copy of lstm網路.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uJCk0HTeoTZnRj-FUqtx-FeZZ8uggg7
"""

import tensorflow.compat.v1 as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
tf.disable_v2_behavior()

batch_size = 7
hidden_layer = 256
clip_margin = 5
learning_rate = 0.001
epochs = 200
time_step=10
train_X,train_Y=[],[]   #訓練集
test_x,test_y=[],[]

tesla_stocks = pd.read_csv('3037_2010_2019_csv.csv')
data_to_use = tesla_stocks['close'].values

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data_to_use.reshape(-1, 1))

len(scaled_data)

def window_data(data, window_size):
    X = []
    y = []
    i = 0
    while (i + window_size) < len(data):
        X.append(data[i : i + window_size])
        y.append(data[i + window_size])     
        i += 1
    assert len(X) ==  len(y)
    return X, y

window_size = 7
X, y = window_data(scaled_data, window_size)

X_train = np.array(X[:2316])#全-window_size-test_data_size-1 = 2316
y_train = np.array(y[:2316])
X_test = np.array(X[2316:])
y_test = np.array(y[2316:])
print("X_train size: {}".format(X_train.shape))
print("y_train size: {}".format(y_train.shape))
print("X_test size: {}".format(X_test.shape))
print("y_test size: {}".format(y_test.shape))

inputs = tf.placeholder(tf.float32, [batch_size, window_size, 1])
targets = tf.placeholder(tf.float32, [batch_size, 1])

#Weights for the input gate
weights_input_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev = 0.05))
weights_input_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev = 0.05))
bias_input = tf.Variable(tf.zeros([hidden_layer]))

#weights for the forgot gate
weights_forget_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev = 0.05))
weights_forget_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev = 0.05))
bias_forget = tf.Variable(tf.zeros([hidden_layer]))

#weights for the output gate
weights_output_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev = 0.05))
weights_output_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev = 0.05))
bias_output = tf.Variable(tf.zeros([hidden_layer]))

#weights for the memory cell
weights_memory_cell = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev = 0.05))
weights_memory_cell_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev = 0.05))
bias_memory_cell = tf.Variable(tf.zeros([hidden_layer]))

## Output layer weigts
weights_output = tf.Variable(tf.truncated_normal([hidden_layer, 1], stddev = 0.05))
bias_output_layer = tf.Variable(tf.zeros([1]))

def LSTM_cell(input, state, output):
    
    input_gate = tf.sigmoid(tf.matmul(input, weights_input_gate) + tf.matmul(output, weights_input_hidden) + bias_input)
    
    forget_gate = tf.sigmoid(tf.matmul(input, weights_forget_gate) + tf.matmul(output, weights_forget_hidden) + bias_forget)
    
    output_gate = tf.sigmoid(tf.matmul(input, weights_output_gate) + tf.matmul(output, weights_output_hidden) + bias_output)
    
    memory_cell = tf.tanh(tf.matmul(input, weights_memory_cell) + tf.matmul(output, weights_memory_cell_hidden) + bias_memory_cell)
    
    state = state * forget_gate + input_gate * memory_cell
    
    output = output_gate * tf.tanh(state)
    return state, output

outputs = []
for i in range(batch_size):
    batch_state = np.zeros([1, hidden_layer], dtype = np.float32) 
    batch_output = np.zeros([1, hidden_layer], dtype = np.float32)
    for j in range(window_size):
        batch_state, batch_output = LSTM_cell(tf.reshape(inputs[i][j], (-1, 1)), batch_state, batch_output)
    outputs.append(tf.matmul(batch_output, weights_output) + bias_output_layer)

losses = []

for i in range(len(outputs)):
    losses.append(tf.losses.mean_squared_error(tf.reshape(targets[i], (-1, 1)), outputs[i]))
    
loss = tf.reduce_mean(losses)

gradients = tf.gradients(loss, tf.trainable_variables())
clipped, _ = tf.clip_by_global_norm(gradients, clip_margin)
optimizer = tf.train.AdamOptimizer(learning_rate)
trained_optimizer = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))

session = tf.Session()

session.run(tf.global_variables_initializer())

for i in range(epochs):
    traind_scores = []
    ii = 0
    epoch_loss = []
    while(ii + batch_size) <= len(X_train):
        X_batch = X_train[ii:ii+batch_size]
        y_batch = y_train[ii:ii+batch_size]
        
        o, c, _ = session.run([outputs, loss, trained_optimizer], feed_dict={inputs:X_batch, targets:y_batch})
        
        epoch_loss.append(c)
        traind_scores.append(o)
        ii += batch_size
    if (i % 30) == 0:
        print('Epoch {}/{}'.format(i, epochs), ' Current loss: {}'.format(np.mean(epoch_loss)))

sup =[]
for i in range(len(traind_scores)):
    for j in range(len(traind_scores[i])):
        sup.append(traind_scores[i][j][0])

X_test.shape

len(tests)

tests = []
i = 0
while i+batch_size <= len(X_test):
    
    o = session.run([outputs], feed_dict={inputs:X_test[i:i+batch_size]})
    i += batch_size
    tests.append(o)

tests_new = []
for i in range(len(tests)):
    for j in range(len(tests[i][0])):
        tests_new.append(tests[i][0][j])

tests

test_results = []
for i in range(2365):
    if i >= 2316:
        test_results.append(tests_new[i-2316])
    else:
        test_results.append(None)

test_results_re = []
for i in range(2365):
    if i >= 2316:
        test_results_re.append(scaler.inverse_transform(tests_new[i-2316]))
    else:
        test_results_re.append(None)

tests_count=[]
for i in range(2365):#test_size+train_size
    if i >= 2316:#test_size+1
      tests_count.append(scaler.inverse_transform(tests_new[i-2316]))

len(test_results)

test_count = np.asarray(tests_count).reshape(49,1)
test_count

real_data_test = scaler.inverse_transform(y_test[np.logical_not(np.isnan(y_test))].reshape(49,1))
real_data_test

test_count

acc = np.sum(np.abs(list(real_data_test[i] - tests_count[i] for i in range(len(real_data_test)))))/len(real_data_test)
 acc

plt.figure(figsize=(16, 7))
plt.plot(scaled_data, label='Original data')
plt.plot(sup, label='Training data')
plt.plot(test_results, label='Testing data')
plt.legend()
plt.show()

plt.figure(figsize=(16, 7))
plt.plot(data_to_use, label='Original data')
plt.plot(scaler.inverse_transform(sup), label='Training data')
plt.plot(test_results_re, label='Testing data')
plt.legend()
plt.show()

# session.close()